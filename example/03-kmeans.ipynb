{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Example\n",
    "\n",
    "&emsp; Show capabilities processing iterative or recursive algorithms\n",
    "- Wayang API allows working with cycles\n",
    "- Enumerating process review if an operator was already processed\n",
    "\n",
    "&emsp; Review in detail how to work with Wayang API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Preparing dependancies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step imports the required modules to execute the code. All these packages come from the previous Maven Instalation\n",
    "\n",
    "```scala\n",
    "import $ivy.`ORGANIZATION:MODULE:VERSION` will bring the exact version\n",
    "import $ivy.`ORGANIZATION::MODULE:VERSION` the \"::\" bring the version of scala needed\n",
    "```\n",
    "\n",
    "The imported libraries are:\n",
    "\n",
    "Module | Java's | Scala's | Description\n",
    ":----- | -------------: | --------------: | :----------\n",
    "wayang-core | 8, 11 | 2.11, 2.12 | provides core data structures and the optimizer (required)\n",
    "wayang-basic | 8, 11 | 2.11, 2.12 | provides common operators and data types for your apps (recommended)\n",
    "wayang-api-scala-java | 8, 11 | 2.11, 2.12 | provides an easy-to-use Scala and Java API to assemble wayang plans (recommended)\n",
    "wayang-spark | 8, 11 | 2.11, 2.12 | adapters for [Apache Spark](https://spark.apache.org) processing platforms\n",
    "hadoop-common | 8,11 | - | Hadoop-commons is required because the lack of the Environment Variable **HADOOP_HOME**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                     \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                          \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* Import Dependencies */\n",
    "import $ivy.`com.fasterxml.jackson.module:jackson-module-scala_2.12:2.10.2`\n",
    "import $ivy.`com.fasterxml.jackson.core:jackson-databind:2.10.2`\n",
    "import $ivy.`org.apache.wayang:wayang-core:0.7.1`\n",
    "import $ivy.`org.apache.wayang:wayang-basic:0.7.1`\n",
    "import $ivy.`org.apache.wayang:wayang-java:0.7.1`\n",
    "import $ivy.`org.apache.wayang:wayang-api-scala-java_2.12:0.7.1`\n",
    "import $ivy.`org.apache.wayang:wayang-spark_2.12:0.7.1`\n",
    "import $ivy.`org.apache.hadoop:hadoop-common:2.8.5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Import Class Section\n",
    "\n",
    "The classes that are required to run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.wayang.api._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.wayang.core.api.{Configuration, WayangContext}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.wayang.core.util.fs.FileSystems\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.wayang.core.function.ExecutionContext\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.wayang.core.function.FunctionDescriptor.ExtendedSerializableFunction\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.wayang.core.plugin.Plugin\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.wayang.spark.Spark\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io.File\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.util.{Collection => JavaCollection}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.JavaConversions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Random\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.wayang.api._\n",
    "import org.apache.wayang.core.api.{Configuration, WayangContext}\n",
    "import org.apache.wayang.core.util.fs.FileSystems\n",
    "import org.apache.wayang.core.function.ExecutionContext\n",
    "import org.apache.wayang.core.function.FunctionDescriptor.ExtendedSerializableFunction\n",
    "import org.apache.wayang.core.plugin.Plugin\n",
    "import org.apache.wayang.spark.Spark\n",
    "import java.io.File\n",
    "import java.util.{Collection => JavaCollection}\n",
    "import scala.collection.JavaConversions._\n",
    "import scala.util.Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mtrait\u001b[39m \u001b[36mPointLike\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mPoint\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mTaggedPoint\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mTaggedPointCounter\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/**\n",
    "  * Represents objects with an x and a y coordinate.\n",
    "  */\n",
    "sealed trait PointLike {\n",
    "\n",
    "  /**\n",
    "    * @return the x coordinate\n",
    "    */\n",
    "  def x: Double\n",
    "\n",
    "  /**\n",
    "    * @return the y coordinate\n",
    "    */\n",
    "  def y: Double\n",
    "\n",
    "}\n",
    "\n",
    "/**\n",
    "  * Represents a two-dimensional point.\n",
    "  *\n",
    "  * @param x the x coordinate\n",
    "  * @param y the y coordinate\n",
    "  */\n",
    "case class Point(x: Double, y: Double) extends PointLike {\n",
    "\n",
    "  /**\n",
    "    * Calculates the Euclidean distance to another [[Point]].\n",
    "    *\n",
    "    * @param that the other [[PointLike]]\n",
    "    * @return the Euclidean distance\n",
    "    */\n",
    "  def distanceTo(that: PointLike) = {\n",
    "    val dx = this.x - that.x\n",
    "    val dy = this.y - that.y\n",
    "    math.sqrt(dx * dx + dy * dy)\n",
    "  }\n",
    "\n",
    "  override def toString: String = f\"($x%.2f, $y%.2f)\"\n",
    "}\n",
    "\n",
    "/**\n",
    "  * Represents a two-dimensional point with a centroid ID attached.\n",
    "  */\n",
    "case class TaggedPoint(x: Double, y: Double, centroidId: Int) extends PointLike {\n",
    "\n",
    "  /**\n",
    "    * Creates a [[Point]] from this instance.\n",
    "    *\n",
    "    * @return the [[Point]]\n",
    "    */\n",
    "  def toPoint = Point(x, y)\n",
    "\n",
    "}\n",
    "\n",
    "/**\n",
    "  * Represents a two-dimensional point with a centroid ID and a counter attached.\n",
    "  */\n",
    "case class TaggedPointCounter(x: Double, y: Double, centroidId: Int, count: Int = 1) extends PointLike {\n",
    "\n",
    "  def this(point: PointLike, centroidId: Int, count: Int) = this(point.x, point.y, centroidId, count)\n",
    "\n",
    "  /**\n",
    "    * Adds coordinates and counts of two instances.\n",
    "    *\n",
    "    * @param that the other instance\n",
    "    * @return the sum\n",
    "    */\n",
    "  def +(that: TaggedPointCounter) = TaggedPointCounter(this.x + that.x, this.y + that.y, this.centroidId, this.count + that.count)\n",
    "\n",
    "  /**\n",
    "    * Calculates the average of all added instances.\n",
    "    *\n",
    "    * @return a [[TaggedPoint]] reflecting the average\n",
    "    */\n",
    "  def average = TaggedPoint(x / count, y / count, centroidId)\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mSelectNearestCentroid\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/**\n",
    "  * UDF to select the closest centroid for a given [[Point]].\n",
    "  */\n",
    "class SelectNearestCentroid extends ExtendedSerializableFunction[Point, TaggedPointCounter] {\n",
    "\n",
    "  /** Keeps the broadcasted centroids. */\n",
    "  var centroids: JavaCollection[TaggedPoint] = _\n",
    "\n",
    "  override def open(executionCtx: ExecutionContext) = {\n",
    "    centroids = executionCtx.getBroadcast[TaggedPoint](\"centroids\")\n",
    "  }\n",
    "\n",
    "  override def apply(point: Point): TaggedPointCounter = {\n",
    "    var minDistance = Double.PositiveInfinity\n",
    "    var nearestCentroidId = -1\n",
    "    for (centroid <- centroids) {\n",
    "      val distance = point.distanceTo(centroid)\n",
    "      if (distance < minDistance) {\n",
    "        minDistance = distance\n",
    "        nearestCentroidId = centroid.centroidId\n",
    "      }\n",
    "    }\n",
    "    new TaggedPointCounter(point, nearestCentroidId, 1)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcreateRandomCentroids\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/**\n",
    "* Creates random centroids.\n",
    "*\n",
    "* @param n      the number of centroids to create\n",
    "* @param random used to draw random coordinates\n",
    "* @return the centroids\n",
    "*/\n",
    "def createRandomCentroids(n: Int, random: Random = new Random()) =\n",
    "  // NOTE: The random cluster ID makes collisions during resurrection \n",
    "  //       less likely but in general permits ID collisions.\n",
    "  for (i <- 1 to n) yield TaggedPoint(random.nextGaussian(), random.nextGaussian(), random.nextInt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36minputFile\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"file:/home/jovyan/files/census.txt\"\u001b[39m\n",
       "\u001b[36mk\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m100\u001b[39m\n",
       "\u001b[36miterations\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m10\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inputFile = new File(\"files/census.txt\").toURI().toString()\n",
    "val k = 100\n",
    "val iterations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"jp-RenderedText\">\n",
       "<pre><code><span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">plugin</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Plugin</span></span> = org.apache.wayang.spark.plugin.SparkBasicPlugin@23080117</code></pre>\n",
       "</div>"
      ],
      "text/plain": [
       "\u001b[36mplugin\u001b[39m: \u001b[32mPlugin\u001b[39m = org.apache.wayang.spark.plugin.SparkBasicPlugin@23080117"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var plugin: Plugin = Spark.basicPlugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mKmeans\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Kmeans(field0: Int, field1: Int) extends Serializable {\n",
    "    \n",
    "  def apply(plugin: Plugin, k: Int, inputFile: String, iterations: Int): Iterable[Point] = {\n",
    "    val context = new WayangContext().withPlugin(plugin)\n",
    "    val planBuilder = new PlanBuilder(context)\n",
    "      \n",
    "    // Read and parse the input file(s).\n",
    "    val points = planBuilder\n",
    "      .readTextFile(inputFile)\n",
    "      .filter( line => line.trim.nonEmpty && !line.startsWith(\"caseid\") )\n",
    "      .map { line =>\n",
    "        val fields = line.split(\",\")\n",
    "        Point(fields(field0).toDouble, fields(field1).toDouble)\n",
    "      }\n",
    "\n",
    "    // Create initial centroids.\n",
    "    val initialCentroids = planBuilder\n",
    "      .loadCollection(createRandomCentroids(k))\n",
    "\n",
    "    // Do the k-means loop.\n",
    "    val finalCentroids = initialCentroids.repeat(iterations, { currentCentroids =>\n",
    "      points\n",
    "        .mapJava(\n",
    "          new SelectNearestCentroid\n",
    "        )\n",
    "        .withBroadcast(currentCentroids, \"centroids\")\n",
    "        .reduceByKey(_.centroidId, _ + _)\n",
    "        .map(_.average)\n",
    "    })\n",
    "\n",
    "    // Collect the result.\n",
    "    finalCentroids\n",
    "      .map(_.toPoint)\n",
    "      .collect()\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.30/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "24/02/05 16:04:06 INFO SparkContext: Running Spark version 3.1.2\n",
      "24/02/05 16:04:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/05 16:04:06 INFO ResourceUtils: ==============================================================\n",
      "24/02/05 16:04:06 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/02/05 16:04:06 INFO ResourceUtils: ==============================================================\n",
      "24/02/05 16:04:06 INFO SparkContext: Submitted application: Wayang app\n",
      "24/02/05 16:04:06 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/02/05 16:04:06 INFO ResourceProfile: Limiting resource is cpu\n",
      "24/02/05 16:04:06 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/02/05 16:04:06 INFO SecurityManager: Changing view acls to: jovyan\n",
      "24/02/05 16:04:06 INFO SecurityManager: Changing modify acls to: jovyan\n",
      "24/02/05 16:04:06 INFO SecurityManager: Changing view acls groups to: \n",
      "24/02/05 16:04:06 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/02/05 16:04:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()\n",
      "24/02/05 16:04:07 INFO Utils: Successfully started service 'sparkDriver' on port 34995.\n",
      "24/02/05 16:04:07 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/02/05 16:04:07 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/02/05 16:04:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/02/05 16:04:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/02/05 16:04:07 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/02/05 16:04:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-75ee05e5-a1ec-4581-b2c2-4912340f37ec\n",
      "24/02/05 16:04:07 INFO MemoryStore: MemoryStore started with capacity 2.1 GiB\n",
      "24/02/05 16:04:07 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/02/05 16:04:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/02/05 16:04:07 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "24/02/05 16:04:07 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://024f5179930a:4041\n",
      "24/02/05 16:04:07 INFO Executor: Starting executor ID driver on host 024f5179930a\n",
      "24/02/05 16:04:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46137.\n",
      "24/02/05 16:04:07 INFO NettyBlockTransferService: Server created on 024f5179930a:46137\n",
      "24/02/05 16:04:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/02/05 16:04:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 024f5179930a, 46137, None)\n",
      "24/02/05 16:04:07 INFO BlockManagerMasterEndpoint: Registering block manager 024f5179930a:46137 with 2.1 GiB RAM, BlockManagerId(driver, 024f5179930a, 46137, None)\n",
      "24/02/05 16:04:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 024f5179930a, 46137, None)\n",
      "24/02/05 16:04:07 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 024f5179930a, 46137, None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fs.s3.awsAccessKeyId\n",
      "fs.s3.awsSecretAccessKey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/05 16:04:08 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 175.9 KiB, free 2.1 GiB)\n",
      "24/02/05 16:04:08 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.1 KiB, free 2.1 GiB)\n",
      "24/02/05 16:04:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 024f5179930a:46137 (size: 27.1 KiB, free: 2.1 GiB)\n",
      "24/02/05 16:04:08 INFO SparkContext: Created broadcast 0 from textFile at SparkTextFileSource.java:70\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.wayang.core.api.exception.WayangException: Executing T[SparkCache[convert out@SparkMap[1->1, id=7d497859]]] failed.\u001b[39m\n  org.apache.wayang.spark.execution.SparkExecutor.execute(\u001b[32mSparkExecutor.java\u001b[39m:\u001b[32m124\u001b[39m)\n  org.apache.wayang.core.platform.PushExecutorTemplate.execute(\u001b[32mPushExecutorTemplate.java\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.wayang.core.platform.PushExecutorTemplate.access$100(\u001b[32mPushExecutorTemplate.java\u001b[39m:\u001b[32m47\u001b[39m)\n  org.apache.wayang.core.platform.PushExecutorTemplate$StageExecution.execute(\u001b[32mPushExecutorTemplate.java\u001b[39m:\u001b[32m195\u001b[39m)\n  org.apache.wayang.core.platform.PushExecutorTemplate$StageExecution.doExecute(\u001b[32mPushExecutorTemplate.java\u001b[39m:\u001b[32m166\u001b[39m)\n  org.apache.wayang.core.util.OneTimeExecutable.tryExecute(\u001b[32mOneTimeExecutable.java\u001b[39m:\u001b[32m41\u001b[39m)\n  org.apache.wayang.core.util.OneTimeExecutable.execute(\u001b[32mOneTimeExecutable.java\u001b[39m:\u001b[32m54\u001b[39m)\n  org.apache.wayang.core.platform.PushExecutorTemplate$StageExecution.executeStage(\u001b[32mPushExecutorTemplate.java\u001b[39m:\u001b[32m156\u001b[39m)\n  org.apache.wayang.core.platform.PushExecutorTemplate.execute(\u001b[32mPushExecutorTemplate.java\u001b[39m:\u001b[32m61\u001b[39m)\n  org.apache.wayang.core.platform.CrossPlatformExecutor.execute(\u001b[32mCrossPlatformExecutor.java\u001b[39m:\u001b[32m378\u001b[39m)\n  org.apache.wayang.core.platform.CrossPlatformExecutor.executeSingleStage(\u001b[32mCrossPlatformExecutor.java\u001b[39m:\u001b[32m248\u001b[39m)\n  org.apache.wayang.core.platform.CrossPlatformExecutor.runToBreakpoint(\u001b[32mCrossPlatformExecutor.java\u001b[39m:\u001b[32m320\u001b[39m)\n  org.apache.wayang.core.platform.CrossPlatformExecutor.executeUntilBreakpoint(\u001b[32mCrossPlatformExecutor.java\u001b[39m:\u001b[32m156\u001b[39m)\n  org.apache.wayang.core.api.Job.execute(\u001b[32mJob.java\u001b[39m:\u001b[32m524\u001b[39m)\n  org.apache.wayang.core.api.Job.doExecute(\u001b[32mJob.java\u001b[39m:\u001b[32m309\u001b[39m)\n  org.apache.wayang.core.util.OneTimeExecutable.tryExecute(\u001b[32mOneTimeExecutable.java\u001b[39m:\u001b[32m41\u001b[39m)\n  org.apache.wayang.core.util.OneTimeExecutable.execute(\u001b[32mOneTimeExecutable.java\u001b[39m:\u001b[32m54\u001b[39m)\n  org.apache.wayang.core.api.Job.execute(\u001b[32mJob.java\u001b[39m:\u001b[32m244\u001b[39m)\n  org.apache.wayang.core.api.WayangContext.execute(\u001b[32mWayangContext.java\u001b[39m:\u001b[32m120\u001b[39m)\n  org.apache.wayang.core.api.WayangContext.execute(\u001b[32mWayangContext.java\u001b[39m:\u001b[32m108\u001b[39m)\n  org.apache.wayang.api.PlanBuilder.buildAndExecute(\u001b[32mPlanBuilder.scala\u001b[39m:\u001b[32m105\u001b[39m)\n  org.apache.wayang.api.DataQuanta.collect(\u001b[32mDataQuanta.scala\u001b[39m:\u001b[32m758\u001b[39m)\n  ammonite.$sess.cmd7$Helper$Kmeans.apply(\u001b[32mcmd7.sc\u001b[39m:\u001b[32m33\u001b[39m)\n  ammonite.$sess.cmd8$Helper.<init>(\u001b[32mcmd8.sc\u001b[39m:\u001b[32m1\u001b[39m)\n  ammonite.$sess.cmd8$.<init>(\u001b[32mcmd8.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd8$.<clinit>(\u001b[32mcmd8.sc\u001b[39m:\u001b[32m-1\u001b[39m)\n\u001b[31morg.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/jovyan/files/census.txt\u001b[39m\n  org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(\u001b[32mFileInputFormat.java\u001b[39m:\u001b[32m297\u001b[39m)\n  org.apache.hadoop.mapred.FileInputFormat.listStatus(\u001b[32mFileInputFormat.java\u001b[39m:\u001b[32m239\u001b[39m)\n  org.apache.hadoop.mapred.FileInputFormat.getSplits(\u001b[32mFileInputFormat.java\u001b[39m:\u001b[32m325\u001b[39m)\n  org.apache.spark.rdd.HadoopRDD.getPartitions(\u001b[32mHadoopRDD.scala\u001b[39m:\u001b[32m205\u001b[39m)\n  org.apache.spark.rdd.RDD.$anonfun$partitions$2(\u001b[32mRDD.scala\u001b[39m:\u001b[32m300\u001b[39m)\n  scala.Option.getOrElse(\u001b[32mOption.scala\u001b[39m:\u001b[32m189\u001b[39m)\n  org.apache.spark.rdd.RDD.partitions(\u001b[32mRDD.scala\u001b[39m:\u001b[32m296\u001b[39m)\n  org.apache.spark.rdd.MapPartitionsRDD.getPartitions(\u001b[32mMapPartitionsRDD.scala\u001b[39m:\u001b[32m49\u001b[39m)\n  org.apache.spark.rdd.RDD.$anonfun$partitions$2(\u001b[32mRDD.scala\u001b[39m:\u001b[32m300\u001b[39m)\n  scala.Option.getOrElse(\u001b[32mOption.scala\u001b[39m:\u001b[32m189\u001b[39m)\n  org.apache.spark.rdd.RDD.partitions(\u001b[32mRDD.scala\u001b[39m:\u001b[32m296\u001b[39m)\n  org.apache.spark.rdd.MapPartitionsRDD.getPartitions(\u001b[32mMapPartitionsRDD.scala\u001b[39m:\u001b[32m49\u001b[39m)\n  org.apache.spark.rdd.RDD.$anonfun$partitions$2(\u001b[32mRDD.scala\u001b[39m:\u001b[32m300\u001b[39m)\n  scala.Option.getOrElse(\u001b[32mOption.scala\u001b[39m:\u001b[32m189\u001b[39m)\n  org.apache.spark.rdd.RDD.partitions(\u001b[32mRDD.scala\u001b[39m:\u001b[32m296\u001b[39m)\n  org.apache.spark.rdd.MapPartitionsRDD.getPartitions(\u001b[32mMapPartitionsRDD.scala\u001b[39m:\u001b[32m49\u001b[39m)\n  org.apache.spark.rdd.RDD.$anonfun$partitions$2(\u001b[32mRDD.scala\u001b[39m:\u001b[32m300\u001b[39m)\n  scala.Option.getOrElse(\u001b[32mOption.scala\u001b[39m:\u001b[32m189\u001b[39m)\n  org.apache.spark.rdd.RDD.partitions(\u001b[32mRDD.scala\u001b[39m:\u001b[32m296\u001b[39m)\n  org.apache.spark.rdd.MapPartitionsRDD.getPartitions(\u001b[32mMapPartitionsRDD.scala\u001b[39m:\u001b[32m49\u001b[39m)\n  org.apache.spark.rdd.RDD.$anonfun$partitions$2(\u001b[32mRDD.scala\u001b[39m:\u001b[32m300\u001b[39m)\n  scala.Option.getOrElse(\u001b[32mOption.scala\u001b[39m:\u001b[32m189\u001b[39m)\n  org.apache.spark.rdd.RDD.partitions(\u001b[32mRDD.scala\u001b[39m:\u001b[32m296\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2261\u001b[39m)\n  org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(\u001b[32mRDD.scala\u001b[39m:\u001b[32m1020\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m112\u001b[39m)\n  org.apache.spark.rdd.RDD.withScope(\u001b[32mRDD.scala\u001b[39m:\u001b[32m414\u001b[39m)\n  org.apache.spark.rdd.RDD.foreachPartition(\u001b[32mRDD.scala\u001b[39m:\u001b[32m1018\u001b[39m)\n  org.apache.spark.api.java.JavaRDDLike.foreachPartition(\u001b[32mJavaRDDLike.scala\u001b[39m:\u001b[32m219\u001b[39m)\n  org.apache.spark.api.java.JavaRDDLike.foreachPartition$(\u001b[32mJavaRDDLike.scala\u001b[39m:\u001b[32m218\u001b[39m)\n  org.apache.spark.api.java.AbstractJavaRDDLike.foreachPartition(\u001b[32mJavaRDDLike.scala\u001b[39m:\u001b[32m45\u001b[39m)\n  org.apache.wayang.spark.operators.SparkCacheOperator.evaluate(\u001b[32mSparkCacheOperator.java\u001b[39m:\u001b[32m62\u001b[39m)\n  org.apache.wayang.spark.execution.SparkExecutor.execute(\u001b[32mSparkExecutor.java\u001b[39m:\u001b[32m114\u001b[39m)\n  org.apache.wayang.core.platform.PushExecutorTemplate.execute(\u001b[32mPushExecutorTemplate.java\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.wayang.core.platform.PushExecutorTemplate.access$100(\u001b[32mPushExecutorTemplate.java\u001b[39m:\u001b[32m47\u001b[39m)\n  org.apache.wayang.core.platform.PushExecutorTemplate$StageExecution.execute(\u001b[32mPushExecutorTemplate.java\u001b[39m:\u001b[32m195\u001b[39m)\n  org.apache.wayang.core.platform.PushExecutorTemplate$StageExecution.doExecute(\u001b[32mPushExecutorTemplate.java\u001b[39m:\u001b[32m166\u001b[39m)\n  org.apache.wayang.core.util.OneTimeExecutable.tryExecute(\u001b[32mOneTimeExecutable.java\u001b[39m:\u001b[32m41\u001b[39m)\n  org.apache.wayang.core.util.OneTimeExecutable.execute(\u001b[32mOneTimeExecutable.java\u001b[39m:\u001b[32m54\u001b[39m)\n  org.apache.wayang.core.platform.PushExecutorTemplate$StageExecution.executeStage(\u001b[32mPushExecutorTemplate.java\u001b[39m:\u001b[32m156\u001b[39m)\n  org.apache.wayang.core.platform.PushExecutorTemplate.execute(\u001b[32mPushExecutorTemplate.java\u001b[39m:\u001b[32m61\u001b[39m)\n  org.apache.wayang.core.platform.CrossPlatformExecutor.execute(\u001b[32mCrossPlatformExecutor.java\u001b[39m:\u001b[32m378\u001b[39m)\n  org.apache.wayang.core.platform.CrossPlatformExecutor.executeSingleStage(\u001b[32mCrossPlatformExecutor.java\u001b[39m:\u001b[32m248\u001b[39m)\n  org.apache.wayang.core.platform.CrossPlatformExecutor.runToBreakpoint(\u001b[32mCrossPlatformExecutor.java\u001b[39m:\u001b[32m320\u001b[39m)\n  org.apache.wayang.core.platform.CrossPlatformExecutor.executeUntilBreakpoint(\u001b[32mCrossPlatformExecutor.java\u001b[39m:\u001b[32m156\u001b[39m)\n  org.apache.wayang.core.api.Job.execute(\u001b[32mJob.java\u001b[39m:\u001b[32m524\u001b[39m)\n  org.apache.wayang.core.api.Job.doExecute(\u001b[32mJob.java\u001b[39m:\u001b[32m309\u001b[39m)\n  org.apache.wayang.core.util.OneTimeExecutable.tryExecute(\u001b[32mOneTimeExecutable.java\u001b[39m:\u001b[32m41\u001b[39m)\n  org.apache.wayang.core.util.OneTimeExecutable.execute(\u001b[32mOneTimeExecutable.java\u001b[39m:\u001b[32m54\u001b[39m)\n  org.apache.wayang.core.api.Job.execute(\u001b[32mJob.java\u001b[39m:\u001b[32m244\u001b[39m)\n  org.apache.wayang.core.api.WayangContext.execute(\u001b[32mWayangContext.java\u001b[39m:\u001b[32m120\u001b[39m)\n  org.apache.wayang.core.api.WayangContext.execute(\u001b[32mWayangContext.java\u001b[39m:\u001b[32m108\u001b[39m)\n  org.apache.wayang.api.PlanBuilder.buildAndExecute(\u001b[32mPlanBuilder.scala\u001b[39m:\u001b[32m105\u001b[39m)\n  org.apache.wayang.api.DataQuanta.collect(\u001b[32mDataQuanta.scala\u001b[39m:\u001b[32m758\u001b[39m)\n  ammonite.$sess.cmd7$Helper$Kmeans.apply(\u001b[32mcmd7.sc\u001b[39m:\u001b[32m33\u001b[39m)\n  ammonite.$sess.cmd8$Helper.<init>(\u001b[32mcmd8.sc\u001b[39m:\u001b[32m1\u001b[39m)\n  ammonite.$sess.cmd8$.<init>(\u001b[32mcmd8.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd8$.<clinit>(\u001b[32mcmd8.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "var result = new Kmeans(0, 1).apply(plugin, k, inputFile, iterations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
